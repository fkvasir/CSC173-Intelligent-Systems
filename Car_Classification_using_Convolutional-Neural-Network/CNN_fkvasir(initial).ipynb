{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Classification Using CNN and Transfer Learning\n",
    "\n",
    "### By: Fulgent Kvasir E. Lavesores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the Problem\n",
    "\n",
    "The goal of this project is to build a Convolutional Neural Network (CNN) that classifies car images into their respective makes and models. Using a pretrained model (ResNet50), I aim to fine-tune its layers to adapt it to a dataset containing car images with bounding boxes and class labels. The project seeks to:\n",
    "1. Process the car dataset and prepare it for training.\n",
    "2. Train the CNN model using a combination of transfer learning and custom classification layers.\n",
    "3. Evaluate the model's performance on unseen test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition\n",
    "\n",
    "The dataset used in this project is the **Stanford Cars Dataset**, which includes:\n",
    "- **Training Images**: 8,144 car images, each annotated with bounding boxes and class labels.\n",
    "- **Testing Images**: 8,041 car images with similar annotations.\n",
    "\n",
    "The dataset was loaded using the `scipy.io` library from `.mat` files. Bounding boxes were used to crop images before feeding them into the model.\n",
    "\n",
    "### Source:\n",
    "Stanford Cars Dataset ([https://www.kaggle.com/datasets/jessicali9530/stanford-cars-dataset/data](https://www.kaggle.com/datasets/jessicali9530/stanford-cars-dataset/data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration and Analysis of Data\n",
    "\n",
    "### Key Points:\n",
    "1. **Number of Classes**: 196 car classes.\n",
    "2. **Number of Images**:\n",
    "   - Training: 8,144 images\n",
    "   - Testing: 8,041 images\n",
    "3. **Bounding Boxes**: Each image has a bounding box annotation for cropping the car region.\n",
    "4. **Dataset Distribution**: Class distribution is balanced, with each class having a similar number of images.\n",
    "\n",
    "### Descriptive Statistics:\n",
    "Statistical analysis was performed to understand the data distribution. Key insights include:\n",
    "- The bounding box coordinates vary across the dataset.\n",
    "- The dataset covers a wide range of car models, providing good diversity.\n",
    "\n",
    "### Visualizations:\n",
    "Plots were created to visualize:\n",
    "- Training loss and accuracy over epochs.\n",
    "- Validation performance trends during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Steps:\n",
    "1. **Loading and Parsing Annotations**:\n",
    "   - Annotations were loaded from `.mat` files for training and testing datasets.\n",
    "   - Bounding box coordinates were extracted to crop images.\n",
    "2. **Data Splitting**:\n",
    "   - The training dataset was split into:\n",
    "     - **Training Set**: 80% of the data\n",
    "     - **Validation Set**: 20% of the data\n",
    "3. **Preprocessing**:\n",
    "   - Images were resized to 224x224 pixels (matching ResNet50 input size).\n",
    "   - Pixel values were normalized to the range [0, 1].\n",
    "4. **Data Augmentation**:\n",
    "   - Techniques such as rotation, zooming, and horizontal flipping were applied to the training data.\n",
    "5. **Model Preparation**:\n",
    "   - Custom fully connected layers were added for classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Ustawienia, aby ukryć ostrzeżenia TensorFlow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Wczytanie danych treningowych\n",
    "cars_annos_train = sio.loadmat('../input/standford-cars-dataset-meta/devkit/cars_train_annos.mat')\n",
    "annotations_train = cars_annos_train['annotations']\n",
    "annotations_train = np.transpose(annotations_train)\n",
    "\n",
    "fnames_train = []\n",
    "bboxes_train = []\n",
    "\n",
    "for annotation in annotations_train:\n",
    "    bbox_x1 = annotation[0][0][0][0]\n",
    "    bbox_y1 = annotation[0][1][0][0]\n",
    "    bbox_x2 = annotation[0][2][0][0]\n",
    "    bbox_y2 = annotation[0][3][0][0]\n",
    "    fname = annotation[0][5][0]\n",
    "    car_class = annotation[0][4][0]\n",
    "    bboxes_train.append((fname, bbox_x1, bbox_x2, bbox_y1, bbox_y2, int(car_class[0])))\n",
    "\n",
    "train_meta = pd.DataFrame(bboxes_train, columns=['fnames', 'bbox_x1', 'bbox_x2', 'bbox_y1', 'bbox_y2', 'car_class'])\n",
    "\n",
    "# Wczytanie danych testowych\n",
    "cars_annos_test = sio.loadmat('../input/standford-cars-dataset-meta/cars_test_annos_withlabels (1).mat')\n",
    "annotations_test = cars_annos_test['annotations']\n",
    "annotations_test = np.transpose(annotations_test)\n",
    "\n",
    "fnames_test = []\n",
    "bboxes_test = []\n",
    "\n",
    "for annotation in annotations_test:\n",
    "    bbox_x1 = annotation[0][0][0][0]\n",
    "    bbox_y1 = annotation[0][1][0][0]\n",
    "    bbox_x2 = annotation[0][2][0][0]\n",
    "    bbox_y2 = annotation[0][3][0][0]\n",
    "    fname = annotation[0][5][0]\n",
    "    car_class = annotation[0][4][0]\n",
    "    bboxes_test.append((fname, bbox_x1, bbox_x2, bbox_y1, bbox_y2, int(car_class[0])))\n",
    "\n",
    "test_meta = pd.DataFrame(bboxes_test, columns=['fnames', 'bbox_x1', 'bbox_x2', 'bbox_y1', 'bbox_y2', 'car_class'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podział danych treningowych na zbiór treningowy i walidacyjny\n",
    "train_meta, val_meta = np.split(train_meta.sample(frac=1, random_state=42), [int(0.8 * len(train_meta))])\n",
    "\n",
    "# Tworzenie katalogów dla przetworzonych obrazów\n",
    "os.makedirs('/kaggle/working/training', exist_ok=True)\n",
    "os.makedirs('/kaggle/working/validation', exist_ok=True)\n",
    "os.makedirs('/kaggle/working/testing', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przetwarzanie i zapisywanie obrazów treningowych\n",
    "for i in range(len(train_meta)):\n",
    "    img_path = f\"../input/stanford-cars-dataset/cars_train/cars_train/{train_meta['fnames'].iloc[i]}\"\n",
    "    left, top, right, bottom = train_meta['bbox_x1'].iloc[i], train_meta['bbox_y1'].iloc[i], train_meta['bbox_x2'].iloc[i], train_meta['bbox_y2'].iloc[i]\n",
    "    if right <= left or bottom <= top:\n",
    "        print(f\"Invalid bbox for image {img_path}: left={left}, top={top}, right={right}, bottom={bottom}\")\n",
    "        continue\n",
    "    img = Image.open(img_path).crop((left, top, right, bottom))\n",
    "    img_output = f\"/kaggle/working/training/{train_meta['fnames'].iloc[i]}\"\n",
    "    img.save(img_output)\n",
    "\n",
    "# Przetwarzanie i zapisywanie obrazów walidacyjnych\n",
    "for i in range(len(val_meta)):\n",
    "    img_path = f\"../input/stanford-cars-dataset/cars_train/cars_train/{val_meta['fnames'].iloc[i]}\"\n",
    "    left, top, right, bottom = val_meta['bbox_x1'].iloc[i], val_meta['bbox_y1'].iloc[i], val_meta['bbox_x2'].iloc[i], val_meta['bbox_y2'].iloc[i]\n",
    "    if right <= left or bottom <= top:\n",
    "        print(f\"Invalid bbox for image {img_path}: left={left}, top={top}, right={right}, bottom={bottom}\")\n",
    "        continue\n",
    "    img = Image.open(img_path).crop((left, top, right, bottom))\n",
    "    img_output = f\"/kaggle/working/validation/{val_meta['fnames'].iloc[i]}\"\n",
    "    img.save(img_output)\n",
    "\n",
    "# Przetwarzanie i zapisywanie obrazów testowych\n",
    "for i in range(len(test_meta)):\n",
    "    img_path = f\"../input/stanford-cars-dataset/cars_test/cars_test/{test_meta['fnames'].iloc[i]}\"\n",
    "    left, top, right, bottom = test_meta['bbox_x1'].iloc[i], test_meta['bbox_y1'].iloc[i], test_meta['bbox_x2'].iloc[i], test_meta['bbox_y2'].iloc[i]\n",
    "    if right <= left or bottom <= top:\n",
    "        print(f\"Invalid bbox for image {img_path}: left={left}, top={top}, right={right}, bottom={bottom}\")\n",
    "        continue\n",
    "    img = Image.open(img_path).crop((left, top, right, bottom))\n",
    "    img_output = f\"/kaggle/working/testing/{test_meta['fnames'].iloc[i]}\"\n",
    "    img.save(img_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aktualizacja ścieżek w DataFrame\n",
    "train_meta['fnames'] = train_meta['fnames'].apply(lambda x: f\"training/{x}\")\n",
    "val_meta['fnames'] = val_meta['fnames'].apply(lambda x: f\"validation/{x}\")\n",
    "test_meta['fnames'] = test_meta['fnames'].apply(lambda x: f\"testing/{x}\")\n",
    "\n",
    "# Konwersja kolumny 'car_class' na string\n",
    "train_meta['car_class'] = train_meta['car_class'].astype(str)\n",
    "val_meta['car_class'] = val_meta['car_class'].astype(str)\n",
    "test_meta['car_class'] = test_meta['car_class'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting / Transforming / Standardizing Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation dla zbioru treningowego\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "training_set = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_meta,\n",
    "    directory=\"/kaggle/working\",\n",
    "    x_col='fnames',\n",
    "    y_col='car_class',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "# Data augmentation dla zbioru walidacyjnego\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validation_set = val_datagen.flow_from_dataframe(\n",
    "    dataframe=val_meta,\n",
    "    directory=\"/kaggle/working\",\n",
    "    x_col='fnames',\n",
    "    y_col='car_class',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizacja danych testowych\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_set = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_meta,\n",
    "    directory=\"/kaggle/working\",\n",
    "    x_col='fnames',\n",
    "    y_col='car_class',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Budowa modelu z użyciem ResNet50\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Odmrażanie ostatnich kilku warstw modelu bazowego\n",
    "for layer in base_model.layers[-10:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(196, activation='softmax')  # 196 klas\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kompilacja modelu z mniejszym współczynnikiem uczenia\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Trening modelu\n",
    "history = model.fit(training_set, validation_data=validation_set, epochs=30, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Wizualizacja wyników\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Wykres strat\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Wykres dokładności\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred, average='macro'),\n",
    "        'Recall': recall_score(y_true, y_pred, average='macro'),\n",
    "        'F1-Score': f1_score(y_true, y_pred, average='macro')\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def plot_metrics(metrics, title='Model Performance'):\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    metrics_names = list(metrics.keys())\n",
    "    values = list(metrics.values())\n",
    "    ax.bar(metrics_names, values, color='skyblue')\n",
    "    ax.set_xlabel('Metrics')\n",
    "    ax.set_ylabel('Scores')\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "\n",
    "def test_model(model, validation_set, test_set):\n",
    "    # Predictions using the dataset objects\n",
    "    y_pred_val = np.argmax(model.predict(validation_set), axis=-1)\n",
    "    y_pred_test = np.argmax(model.predict(test_set), axis=-1)\n",
    "\n",
    "    # Extracting true labels from your dataset\n",
    "    y_true_val = np.concatenate([y for x, y in validation_set], axis=0)\n",
    "    y_true_test = np.concatenate([y for x, y in test_set], axis=0)\n",
    "\n",
    "    # Evaluate\n",
    "    val_metrics = evaluate_model(y_true_val, y_pred_val)\n",
    "    test_metrics = evaluate_model(y_true_test, y_pred_test)\n",
    "\n",
    "    # Plot results\n",
    "    plot_metrics(val_metrics, title='Validation Set Performance')\n",
    "    plot_metrics(test_metrics, title='Test Set Performance')\n",
    "\n",
    "# Replace 'history' with 'model', assuming 'model' is your trained model object\n",
    "test_model(model, validation_set, test_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualizations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
